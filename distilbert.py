# -*- coding: utf-8 -*-
"""DistilBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/154dQEtPvMrPv-KV2UV29eU8sMm1D3i6c
"""

import random
import numpy as np
import torch
from transformers import set_seed

seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
set_seed(seed)

# Colab / fresh env
!pip install -q transformers datasets accelerate evaluate tokenizers
!pip install -q sentencepiece  # sometimes useful

import pandas as pd
import numpy as np
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset, DatasetDict
import evaluate

# Loading Dataset, importing export Labels

df = pd.read_csv("student_feedback_labeled.csv")  # adjust path if needed
print(df.shape)
print(df['sentiment'].value_counts())

# Drop missing text rows (defensive)
df = df.dropna(subset=['clean_text']).reset_index(drop=True)

# Encode labels to integers
le = LabelEncoder()
df['label_id'] = le.fit_transform(df['sentiment'])
label2id = {label: int(idx) for idx, label in enumerate(le.classes_)}
id2label = {int(idx): label for idx, label in enumerate(le.classes_)}

print("label2id:", label2id)

"""#Train / val / test split and build Hugging Face `datasets`"""

train_df, test_df = train_test_split(df, test_size=0.15, stratify=df['label_id'], random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.1765, stratify=train_df['label_id'], random_state=42)
# These sizes give roughly 70/15/15 overall: adjust as you like.

# Convert to datasets
train_ds = Dataset.from_pandas(train_df[['clean_text','label_id']])
val_ds   = Dataset.from_pandas(val_df[['clean_text','label_id']])
test_ds  = Dataset.from_pandas(test_df[['clean_text','label_id']])

dataset = DatasetDict({"train": train_ds, "validation": val_ds, "test": test_ds})
print(dataset)

"""# Tokeniser and Tokenisation function

"""

model_name = "distilbert-base-uncased"
tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)

max_length = 128  # try 128 first; raise if truncating important content
def tokenize_fn(batch):
    return tokenizer(batch['clean_text'], truncation=True, padding=False, max_length=max_length)

# Map the tokenization across the dataset (batched)
tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=["clean_text"])
tokenized = tokenized.rename_column("label_id", "labels")
tokenized.set_format("torch")

"""# Data Collator for Dynamic Padding"""

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

"""# Build model with correct label mapping"""

num_labels = len(le.classes_)
model = DistilBertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id
)

"""# Define metrics function

We'll compute accuracy and macro precision/recall/F1
"""

metric_accuracy = evaluate.load("accuracy")
metric_f1 = evaluate.load("f1")
metric_precision = evaluate.load("precision")
metric_recall = evaluate.load("recall")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = metric_accuracy.compute(predictions=preds, references=labels)
    f1 = metric_f1.compute(predictions=preds, references=labels, average="macro")
    prec = metric_precision.compute(predictions=preds, references=labels, average="macro")
    rec = metric_recall.compute(predictions=preds, references=labels, average="macro")
    return {"accuracy": acc["accuracy"], "f1_macro": f1["f1"], "precision_macro": prec["precision"], "recall_macro": rec["recall"]}

"""# TrainingArguments — pick sensible defaults"""

output_dir = "./distilbert_finetuned"

import transformers
print(f"Transformers version: {transformers.__version__}")

training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,  # reduce if OOM
    per_device_eval_batch_size=32,
    num_train_epochs=3,              # start with 3
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",  # choose macro F1 for class imbalance
    greater_is_better=True,
    fp16=torch.cuda.is_available(),   # use mixed precision if GPU supports it
    gradient_accumulation_steps=1,
    logging_steps=50,
    seed=42
)

"""# Create Trainer and start training"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()

"""# Evaluate on Test set"""

result = trainer.evaluate(eval_dataset=tokenized["test"])
print(result)

"""Producing confusion matrix and classification report (for interpretability):"""

from sklearn.metrics import classification_report, confusion_matrix
preds_logits = trainer.predict(tokenized["test"]).predictions
preds = np.argmax(preds_logits, axis=-1)
labels = np.array(tokenized["test"]["labels"])
print(classification_report(labels, preds, target_names=le.classes_))
print(confusion_matrix(labels, preds))

"""# Saving model + tokenizer (and label encoder mapping)"""

trainer.save_model(output_dir)  # saves model + tokenizer config
tokenizer.save_pretrained(output_dir)

# Save mapping files
import json
with open(f"{output_dir}/id2label.json", "w") as f:
    json.dump(id2label, f)
with open(f"{output_dir}/label2id.json", "w") as f:
    json.dump(label2id, f)
# Save LabelEncoder if needed
import joblib
joblib.dump(le, f"{output_dir}/label_encoder.joblib")

"""# Inference: load model and predict on new texts"""

from transformers import pipeline
clf = pipeline("text-classification", model=output_dir, tokenizer=output_dir, device=0 if torch.cuda.is_available() else -1)
examples = [
    "The instructor explained concepts clearly and helped with assignments.",
    "The course was poorly organized and the labs were confusing."
]
preds = clf(examples, truncation=True, max_length=max_length)
print(preds)
# Map label ids back to original label string if necessary (pipeline may return label names)

"""# Things to monitor and tune:
class imbalance
"""

import pandas as pd

df = pd.read_csv("/content/student_feedback_labeled.csv")
print(df['sentiment'].value_counts(normalize=True))  # proportion per class

"""Computing class weights:"""

import torch
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

labels = df['sentiment'].values  # numeric labels: 0,1,2
classes = np.unique(labels)

class_weights = compute_class_weight(
    class_weight="balanced",
    classes=classes,
    y=labels
)

class_weights = torch.tensor(class_weights, dtype=torch.float)
print(class_weights)

"""creating a custom trainer that uses the weighted loss:"""

from transformers import Trainer
import torch.nn as nn

class WeightedTrainer(Trainer):
    def __init__(self, class_weights=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights.to(self.model.device) if class_weights is not None else None

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        if self.class_weights is not None:
            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)
        else:
            loss_fct = nn.CrossEntropyLoss()

        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

"""Using WeightedTrainer instead of trainer:"""

trainer = WeightedTrainer(
    class_weights=class_weights,
    model=model,
    args=training_args,
    train_dataset= tokenized["train"],
    eval_dataset = tokenized["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

"""14.2 Tune learning rate, batch size, epochs, weight decay, fp16
All of these are controlled through TrainingArguments.

Step 14.2.1 – Start with a base TrainingArguments
"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/NLP_Project/distilbert_runs/run_1",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,        # you’ll try 5e-5, 3e-5, 2e-5 across runs
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,        # start with 2–4
    weight_decay=0.01,         # regularization
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",   # if your compute_metrics returns 'f1'
    greater_is_better=True,
    fp16=True                    # if your GPU supports it
)

"""To tune:

Learning rate → rerun with e.g. learning_rate=3e-5, 2e-5.

Batch size → increase if GPU memory allows (e.g. 32).
If you can’t increase, use gradient accumulation:

14.3 Set max_length sensibly

Step 14.3.1 – Inspect token lengths
"""

from tqdm import tqdm

texts = df['clean_text'].astype(str).tolist() # Ensure all elements are strings
encodings = tokenizer(
    texts,
    truncation=True,
    padding=False,
    max_length=128,
    return_length=True
)

lengths = encodings["length"]
import numpy as np
print("Mean length:", np.mean(lengths))
print("95th percentile:", np.percentile(lengths, 95))
print("Max length (after truncation):", max(lengths))

"""In the tokenisation function:"""

def tokenize_batch(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128   # or 256 if needed
    )

"""# 14.4 Add early stopping to avoid overfitting

Step 14.4.1 – Use EarlyStoppingCallback
"""

from transformers import EarlyStoppingCallback

early_stopping = EarlyStoppingCallback(
    early_stopping_patience=2  # stop if eval metric hasn't improved for 2 evals
)

trainer = WeightedTrainer(
    class_weights=class_weights,
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],      # was train_ds
    eval_dataset=tokenized["validation"],  # was val_ds
    tokenizer=tokenizer,
    data_collator=data_collator,           # NEW: use dynamic padding
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

"""# 15) Error analysis & interpretation

15.1 Get predictions on the test set
"""

predictions = trainer.predict(tokenized["test"])
logits = predictions.predictions
y_true = predictions.label_ids

import numpy as np
y_pred = np.argmax(logits, axis=1)

"""15.2 Confusion matrix + classification report"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

print(classification_report(y_true, y_pred, target_names=["negative", "neutral", "positive"]))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d",
            xticklabels=["neg","neu","pos"],
            yticklabels=["neg","neu","pos"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - DistilBERT")
plt.show()

"""15.3 Inspect misclassified examples

You want to read some of the mistakes.

Assuming test_texts is a list of texts aligned with test_dataset:
"""

import pandas as pd

# test_df is already available from previous cells

analysis_df = pd.DataFrame({
    "text": test_df["clean_text"],
    "true_label": y_true,
    "pred_label": y_pred
})

misclassified = analysis_df[analysis_df["true_label"] != analysis_df["pred_label"]]
misclassified.head(10)

"""Then manually scroll through:

Look for sarcasm

Mixed statements (“hard but rewarding”)

Very short comments (“ok”, “fine”)
You can also filter specific patterns:
"""

misclassified[misclassified["true_label"] == 2].head(10)  # true positive misclassified

"""15.4 Word clouds for deeper intuition"""

from wordcloud import WordCloud

pos_text = " ".join(analysis_df[(analysis_df.true_label == 2) & (analysis_df.pred_label == 2)]["text"].tolist())
neg_text = " ".join(analysis_df[(analysis_df.true_label == 0) & (analysis_df.pred_label == 0)]["text"].tolist())

wc_pos = WordCloud(width=800, height=400).generate(pos_text)
wc_neg = WordCloud(width=800, height=400).generate(neg_text)

plt.figure(figsize=(10,5))
plt.imshow(wc_pos)
plt.axis("off")
plt.title("WordCloud - Correctly Predicted Positive")
plt.show()

plt.figure(figsize=(10,5))
plt.imshow(wc_neg)
plt.axis("off")
plt.title("WordCloud - Correctly Predicted Negative")
plt.show()

best_metrics = trainer.evaluate(tokenized["test"])  # or val_dataset

experiment = {
    "run_name": "distilbert_lr5e-5_bs16_epochs3",
    "seed": seed,
    "learning_rate": training_args.learning_rate,
    "train_batch_size": training_args.per_device_train_batch_size,
    "eval_batch_size": training_args.per_device_eval_batch_size,
    "num_epochs": training_args.num_train_epochs,
    "max_length": 128,
    "weight_decay": training_args.weight_decay,
    "class_weights": class_weights.tolist(),
    "accuracy": best_metrics.get("eval_accuracy", None),
    "f1": best_metrics.get("eval_f1_macro", None), # Changed from 'eval_f1' to 'eval_f1_macro'
    "precision": best_metrics.get("eval_precision_macro", None),
    "recall": best_metrics.get("eval_recall_macro", None),
}

"""# Appending to a CSV"""

import os
import pandas as pd

exp_path = "/content/drive/MyDrive/NLP_Project/experiments.csv"

if os.path.exists(exp_path):
    df_exp = pd.read_csv(exp_path)
    df_exp = pd.concat([df_exp, pd.DataFrame([experiment])], ignore_index=True)
else:
    df_exp = pd.DataFrame([experiment])

df_exp.to_csv(exp_path, index=False)